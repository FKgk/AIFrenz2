{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, make_scorer\n",
    "from sklearn.model_selection import KFold\n",
    "from catboost import Pool, CatBoostRegressor, cv\n",
    "\n",
    "# from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mae(y_true, y_pred) :\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "#     y_true = y_true.reshape(1, -1)[0]\n",
    "#     y_pred = y_pred.reshape(1, -1)[0]\n",
    "#     over_threshold = y_true >= 0.1\n",
    "    \n",
    "#     return np.mean(np.abs(y_true[over_threshold] - y_pred[over_threshold]))\n",
    "\n",
    "# def fscore(y_true, y_pred):\n",
    "#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    \n",
    "#     y_true = y_true.reshape(1, -1)[0]\n",
    "#     y_pred = y_pred.reshape(1, -1)[0]\n",
    "#     remove_NAs = y_true >= 0\n",
    "    \n",
    "#     y_true = np.where(y_true[remove_NAs] >= 0.1, 1, 0)\n",
    "#     y_pred = np.where(y_pred[remove_NAs] >= 0.1, 1, 0)\n",
    "    \n",
    "#     return (f1_score(y_true, y_pred))\n",
    "\n",
    "# def maeOverFscore(y_true, y_pred):\n",
    "#     return mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07)\n",
    "\n",
    "# def score(y_val, pred):\n",
    "#     f_value =  fscore(y_val, pred)\n",
    "#     mae_value = maeOverFscore(y_val, pred)\n",
    "#     print(f\"fscore        : {f_value}\")\n",
    "#     print(f\"maeOverFscore : {mae_value}\")\n",
    "    \n",
    "#     return (f_value, mae_value)\n",
    "\n",
    "# def maeOverFscore_lgb(y_true, y_pred):\n",
    "#     return \"maeOverFscore\", mae(y_true, y_pred) / (fscore(y_true, y_pred) + 1e-07), False\n",
    "\n",
    "# def fscore_lgb(y_true, y_pred):\n",
    "#     return \"fscore\", fscore(y_true, y_pred), False\n",
    "\n",
    "# maeOverFscore_sklearn = make_scorer(maeOverFscore)\n",
    "# fscore_sklearn = make_scorer(fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class maeOverFscoreMetric(object):\n",
    "#     def get_final_error(self, error, weight):\n",
    "#         return error / (weight + 1e-07)\n",
    "\n",
    "#     def is_max_optimal(self):\n",
    "#         return False\n",
    "\n",
    "#     def evaluate(self, approxes, target, weight):\n",
    "        \n",
    "#         accuracy_sum = 0\n",
    "#         weight_sum = 0 \n",
    "\n",
    "#         for i in range(len(approxes[0])):\n",
    "#             w = 1.0 if weight is None else weight[i]\n",
    "#             weight_sum += w\n",
    "#             accuracy_sum += w * (best_class[i] == target[i])\n",
    "\n",
    "#         return mae(target, approxes), fscore(target, approxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.getcwd()\n",
    "data_path = os.path.join(base, 'data')\n",
    "submit_path = os.path.join(base, 'submit')\n",
    "model_path = os.path.join(base, 'model')\n",
    "\n",
    "def load_data(name):\n",
    "    return np.load(os.path.join(data_path, f\"{name}.npy\"))\n",
    "\n",
    "def reshape(data):\n",
    "    return data.reshape(data.shape[0] * 40 * 40, data.shape[-1])\n",
    "\n",
    "if not os.path.isdir(model_path):\n",
    "    os.mkdir(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = reshape(load_data('dl_train'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# seperate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "Y = data[:,  -1].reshape(data.shape[0], 1)\n",
    "data = range(data.shape[0])\n",
    "print(X.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Pool(data=X[:36468480, :],\n",
    "               label=Y[:36468480, :],\n",
    "               cat_features=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"iterations\": 1000,\n",
    "          \"depth\": 4,\n",
    "          \"loss_function\": \"MAE\",\n",
    "          \"verbose\": False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cv(dataset,\n",
    "            params,\n",
    "            fold_count=4,\n",
    "            plot=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MetricVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, random_state=7, shuffle=False)\n",
    "scores = list()\n",
    "best_iterations = list()\n",
    "best_scores = list()\n",
    "cat_features = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kfold.split(data)):\n",
    "        train_dataset = Pool(data=X[train_idx, :],\n",
    "                     label=Y[train_idx, :],\n",
    "                     cat_features=cat_features)\n",
    "        \n",
    "        scores = cv(train_dataset,\n",
    "            params,\n",
    "            fold_count=5, \n",
    "            plot=\"True\")\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=4, random_state=7, shuffle=False)\n",
    "scores = list()\n",
    "best_iterations = list()\n",
    "best_scores = list()\n",
    "cat_features = []\n",
    "\n",
    "for i, (train_idx, val_idx) in enumerate(kfold.split(data)):\n",
    "    \n",
    "    train_dataset = Pool(data=X[train_idx, :],\n",
    "                     label=Y[train_idx, :],\n",
    "                     cat_features=cat_features)\n",
    "\n",
    "    eval_dataset = Pool(data=X[val_idx, :],\n",
    "                        label=Y[val_idx, :],\n",
    "                        cat_features=cat_features)\n",
    "    \n",
    "    \n",
    "    clf = CatBoostRegressor(iterations=1000, learning_rate=0.1, \\\n",
    "                            depth=4, l2_leaf_reg=20, \\\n",
    "                            bootstrap_type='Bernoulli', subsample=0.6, \\\n",
    "                            eval_metric='RMSE', metric_period=50, \\\n",
    "                            od_type='Iter', od_wait=45, random_seed=7,\\\n",
    "                            allow_writing_files=True,\n",
    "                            random_state =7)\n",
    "    \n",
    "    clf.fit(train_dataset, \\\n",
    "            eval_set=eval_dataset, \\\n",
    "            use_best_model=True, verbose=True)\n",
    "\n",
    "    \n",
    "    clf.save_model(os.path.join(model_path, f\"cat_{i}\"))\n",
    "    \n",
    "    scores.append(score(clf.predict(X[val_idx, :]), Y[val_idx, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CatBoostRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = CatBoostRegressor(iterations=1000, learning_rate=0.1, \\\n",
    "                             depth=4, l2_leaf_reg=20, \\\n",
    "                             bootstrap_type='Bernoulli', subsample=0.6, \\\n",
    "                             eval_metric='RMSE', metric_period=50, \\\n",
    "                             od_type='Iter', od_wait=45, random_seed=17,\\\n",
    "                             allow_writing_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X, Y, \\\n",
    "        cat_features=[], use_best_model=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "lgb.plot_importance(clf, ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit(clf, name, preprocess=None):\n",
    "    x_test = reshape(load_data('test'))\n",
    "    \n",
    "    pred = clf.predict(x_test)\n",
    "\n",
    "    submission = pd.read_csv(os.path.join(data_path, 'sample_submission.csv'))\n",
    "    submission.iloc[:, 1:] = pred.reshape(-1, 1600)\n",
    "\n",
    "    submission.to_csv(os.path.join(submit_path, f'{name}.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit(clf, 'lightbgm_all_31_800')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://dacon.io/competitions/official/235591/mysubmission/\n",
    "- D:\\인공지능_공모전\\github\\submit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
